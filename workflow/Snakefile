#import stuff
import sys
import os
import pandas as pd
from Bio import SeqIO

#SDIR=os.path.realpath(os.path.dirname(srcdir("env.cfg"))+"/..")
#shell.prefix(f"source {SDIR}/env.cfg ; set -eo pipefail; ")

#workdir: "dup_denebulatizer_output"

rgns = config.keys()

manifest_df = pd.read_csv(config["CYP2D6"]['manifest'], sep = '\t')
manifest_df = manifest_df.set_index(keys = ['sample', 'hap'], drop=False)
sms, haps = zip(*manifest_df.index.values)

wildcard_constraints:
   r="|".join(rgns),
#  sm="|".join( set(manifest_df['sample']) ),
#  h="|".join(set(manifest_df['hap'])),
#  targ = "|".join( expand("{sample}_{hap}", sample = manifest_df['sample'], hap = manifest_df['hap'] ) ),
#  query = "|".join( expand("{sample}_{hap}", sample = manifest_df['sample'], hap = manifest_df['hap'] ) )

def get_manifest_df(wc):
    '''given rgn, return pandas df of manifest of that given sample.'''
    manifest_df = pd.read_csv(config[wc.r]['manifest'], sep = '\t')
    manifest_df = manifest_df.set_index(keys = ['sample', 'hap'], drop=False)
    return manifest_df

def get_hap_fasta(wc):
    '''given sm and h wildcards, return fasta from manifest of sample hap.'''
    manifest_df = get_manifest_df(wc)
    return manifest_df.loc[wc.sm, wc.h]['fasta']

def get_rgn_fasta(wc):
    '''given rgn, return sequence fasta of locus'''
    return config[wc.r]['locus_fasta']

def get_all(wc):
  f="coreDup_beds/{sm}_{h}_CYP2D6_annotation.bed"
  o = []
  for sm, hap in manifest_df.index.values:
      o.append(f.format(sm=sm,h=hap))
  return(o)

rule all:
    input:
       corDup_annot = get_all 
#      corDup_annot = expand("coreDup_beds/{sm}_{h}_{r}_annotation.bed", sm = list(manifest_df['sample']) , h = list(manifest_df['hap']), coreDup = config.get('locus_name') )
#
#rule annotate_cores:
#   '''given clustered graph and haplotype of interest, annotate each core duplicon by graph clustering.'''
#    input:
#      hap_bed = rules.annotate_coreDup.output.core_bed,
#      graph = rules.cluster_graph.output.clustered_graph
#    output:
#      annotated_bed = "{sm}_{h}_{r}_cluster_annotation.bed"

#rule cluster_graph:
#    '''Cluster corDup graph into putative paralogs across sample pop.'''
#    input:
#      #TO DO: figure out if I need to take file or if I can work with something already loaded in environment
#    output:
#    run:

#rule make_graph:
#    '''generate graph using core dup relations.'''
#    input:
#      targ_bed_relations = expand("graph_targ_query_relations/{targ}_{query}_{r}.bed", 
#      targ = get_targ_query_pairs['targets'],
#      query = get_targ_query_pairs['query'], 
#      coreDup = config.coreDup )
#    output:
#      graph_file = #TO DO : figure out graph file format to use.
#    run:


# rule filter_alignments:
# '''process alignments so only dealing with informative alignments'''
#     input:
#     output:

rule align_samples:
   '''generate paf between target and query haplotypes'''
    input:
      combined_fa = "combined_fa/{r}_combined.fa" #rules._rename_combine_fastas.output.cat_fa
    output:
      bam = "{r}_all_vs_all.bam"
    threads: 16
    resources:
      mem_gb=4,
      hrs=4
    conda:
      "envs/align.yaml"
    shell:'''
minimap2 -ax asm10 -X -t {threads} -c --eqx -r 500 -K 500M {input.combined_fa} {input.combined_fa}  | samtools view -b - | samtools sort > {output.bam}
'''

rule get_targ_query_coreDup_relations:
   '''process paf to assign alignment scores between query and target coreDups'''
   input:
       bam = rules.align_samples.output.bam
   output:
       locus_mappings = "{r}/{r}_ref_query_locus_mapping_stats.txt"
   params:
       locus_bed_dir = "coreDup_beds"
   script:
       "scripts/get_allelic_dup_mappings.py"

rule annotate_coreDup:
    '''identify core duplicons in each haplotype.'''
    input:
      hap_asm_fa = get_hap_fasta,
      locus_fa = get_rgn_fasta 
    output:
      core_bed = "coreDup_beds/{sm}_{h}_{r}_annotation.bed"
    threads: 8
    params:
      min_aln_len=3342 #3/4ths length CYP2D6...to change
    conda:
        "envs/align.yaml"
    shell:"""
minimap2 -x asm20 -P -t {threads} -c --eqx -m 100 -r 500 -K 500M {input.locus_fa} {input.hap_asm_fa} | rb filter -a {params.min_aln_len} | cut -f 1,3,4 | awk 'BEGIN{{OFS="\t"}}{{print $0, "{wildcards.r}__{wildcards.sm}_{wildcards.h}__" NR }}'> {output.core_bed} 

"""

rule _rename_combine_fastas:
    '''rename sample contigs to only contain: sample_hap_tig1,2,... and merge to single combined fasta'''
    output:
      old_new_name_map = "tigName_maps/{r}_tig_name_changes.tbl" ,
      cat_fa = "combined_fa/{r}_combined.fa" ,
      temp_fa = temp("temp_{r}.fa")
    params:
      manifest_df = get_manifest_df
    run:
        name_change_df = pd.DataFrame(columns = ['original_fasta', 'original_name', 'new_name'])
        #iterate over each sample row
        for i, row in params.manifest_df.iterrows():
            cur_fasta , cur_samp, cur_hap = row['fasta'], row['sample'], row['hap']
            with open( cur_fasta ) as original_file: #change names, cat to combined file
                cur_name_change_df = pd.DataFrame(columns = ['original_fasta', 'original_name', 'new_name'])
                records = [ i for i in SeqIO.parse(original_file, 'fasta') ] #generate list of fasta sequences
                for i,seq in enumerate(records):
                    cur_name_change_df.loc[i] = [cur_fasta, seq.id, f"{cur_samp}__{cur_hap}__{i}"]
                    seq.id = cur_name_change_df.loc[i, "new_name"]
                    seq.description = "" #description added to end of header. was old name previously.
                with open(output.temp_fa, "w") as output_handle:
                    SeqIO.write(records, output_handle, "fasta")
                os.system(f"cat {output.temp_fa} >> {output.cat_fa}")
            name_change_df = name_change_df.append(cur_name_change_df) #include name change in map file.
        name_change_df.to_csv(output.old_new_name_map, sep='\t' , header = True, index = False) 

rule get_cluster_annotations:
  '''given mappings table from rule get_targ_query_coreDup_relations, produce cluster annotations using louvain clustering and output tbl with paralog-cluster map.'''
  input:
    mapping_stats = rules.get_targ_query_coreDup_relations.output.locus_mappings
  output:
    cluster_annotation = "{r}/{r}_dup_clustering_annotations.txt"
  conda:
    "envs/networkX.yml"
  script:
    "scripts/get_cluster_annotations.py"
