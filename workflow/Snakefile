#import stuff
import sys
import os
import pandas as pd
from Bio import SeqIO

#SDIR=os.path.realpath(os.path.dirname(srcdir("env.cfg"))+"/..")
#shell.prefix(f"source {SDIR}/env.cfg ; set -eo pipefail; ")

#workdir: "dup_denebulatizer_output"

rgns = config.keys()

wildcard_constraints:
   r="|".join(rgns),
#  sm="|".join( set(manifest_df['sample']) ),
#  h="|".join(set(manifest_df['hap'])),
#  targ = "|".join( expand("{sample}_{hap}", sample = manifest_df['sample'], hap = manifest_df['hap'] ) ),
#  query = "|".join( expand("{sample}_{hap}", sample = manifest_df['sample'], hap = manifest_df['hap'] ) )

def get_manifest_df(wc):
    '''given rgn, return pandas df of manifest of that given sample.'''
    manifest_df = pd.read_csv(config[wc.r]['manifest'], sep = '\t')
    manifest_df = manifest_df.set_index(keys = ['sample', 'hap'], drop=False)
    return manifest_df

def get_hap_fasta(wc):
    '''given sm and h wildcards, return fasta from manifest of sample hap.'''
    manifest_df = get_manifest_df(wc)
    return manifest_df.loc[wc.sm, wc.h]['fasta']

def get_rgn_fasta(wc):
    '''given rgn, return sequence fasta of locus'''
    return config[wc.r]['locus_fasta']

#rule all:
#    input:
#      corDup_annot = expand("{sm}_{h}_{r}_cluster_annotation.bed", sm = list(manifest_df['sample']) , h = list(manifest_df['hap']), coreDup = config.get('locus_name') )
#
#rule annotate_cores:
#   '''given clustered graph and haplotype of interest, annotate each core duplicon by graph clustering.'''
#    input:
#      hap_bed = rules.annotate_coreDup.output.core_bed,
#      graph = rules.cluster_graph.output.clustered_graph
#    output:
#      annotated_bed = "{sm}_{h}_{r}_cluster_annotation.bed"

#rule cluster_graph:
#    '''Cluster corDup graph into putative paralogs across sample pop.'''
#    input:
#      #TO DO: figure out if I need to take file or if I can work with something already loaded in environment
#    output:
#    run:

#rule make_graph:
#    '''generate graph using core dup relations.'''
#    input:
#      targ_bed_relations = expand("graph_targ_query_relations/{targ}_{query}_{r}.bed", 
#      targ = get_targ_query_pairs['targets'],
#      query = get_targ_query_pairs['query'], 
#      coreDup = config.coreDup )
#    output:
#      graph_file = #TO DO : figure out graph file format to use.
#    run:

#rule get_targ_query_coreDup_relations:
#    '''process paf to assign alignment scores between query and target coreDups'''
#    input:
#        targ_coreDup_bed = get_bed_path("target") ,
#        query_coreDup_bed = get_bed_path("query")
#    output:
#        graph_tab = "graph_targ_query_relations/{targ}_{query}_{r}.bed"

# rule filter_alignments:
# '''process alignments so only dealing with informative alignments'''
#     input:
#     output:

rule align_samples:
   '''generate paf between target and query haplotypes'''
    input:
      combined_fa = "combined_fa/{r}_combined.fa" #rules._rename_combine_fastas.output.cat_fa
    output:
      paf = "{r}_all_vs_all.paf"
    threads: 16
    resources:
      mem_gb=4,
      hrs=4
    conda:
      "envs/align.yaml"
    shell:'''
minimap2 -x asm10 -X -t {threads} --secondary=yes -p 0.3 -N 10000 --eqx -r 500 -K 500M {input.combined_fa} {input.combined_fa} > {output.paf}
'''

rule annotate_coreDup:
    '''identify core duplicons in each haplotype.'''
    input:
      hap_asm_fa = get_hap_fasta,
      locus_fa = get_rgn_fasta 
    output:
      core_bed = "coreDup_beds/{sm}_{h}_{r}_annotation.bed"
    conda:
        "envs/align.yaml"
    shell:'''
minimap2 -ax asm20 --secondary=yes -p 0.3 -N 10000 --eqx -r 500 -K 500M {input.hap_asm_fa} {input.locus_fa} | \
    samtools view -b - | samtools sort | \
    bedtools bamtobed -i - > {output.core_bed} || touch {output.core_bed}    
'''

rule _rename_combine_fastas:
    '''rename sample contigs to only contain: sample_hap_tig1,2,... and merge to single combined fasta'''
    output:
      old_new_name_map = "tigName_maps/{r}_tig_name_changes" ,
      cat_fa = "combined_fa/{r}_combined.fa" ,
      temp_fa = temp("temp_{r}.fa")
    params:
      manifest_df = get_manifest_df
    run:
        name_change_df = pd.DataFrame(columns = ['original_fasta', 'original_name', 'new_name'])
        #iterate over each sample row
        for i, row in params.manifest_df.iterrows():
            cur_fasta , cur_samp, cur_hap = row['fasta'], row['sample'], row['hap']
            with open( cur_fasta ) as original_file: #change names, cat to combined file
                cur_name_change_df = pd.DataFrame(columns = ['original_fasta', 'original_name', 'new_name'])
                records = [ i for i in SeqIO.parse(original_file, 'fasta') ] #generate list of fasta sequences
                for i,seq in enumerate(records):
                    cur_name_change_df.loc[i] = [cur_fasta, seq.id, f"{cur_samp}__{cur_hap}__{i}"]
                    seq.id = cur_name_change_df.loc[i, "new_name"]
                    seq.description = "" #description added to end of header. was old name previously.
                with open(output.temp_fa, "w") as output_handle:
                    SeqIO.write(records, output_handle, "fasta")
                os.system(f"cat {output.temp_fa} >> {output.cat_fa}")
            name_change_df = name_change_df.append(cur_name_change_df) #include name change in map file.
        name_change_df.to_csv(output.old_new_name_map, sep='\t' , header = True, index = False) 
